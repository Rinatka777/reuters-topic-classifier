{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17dceb2",
   "metadata": {},
   "source": [
    "## Welcome to your notebook\n",
    "\n",
    "Write blocks of text in *markdown* format around your blocks of Python code.\n",
    "\n",
    "#### Downloading NLTK\n",
    "\n",
    "Let's see how we initialize our system by installing NLTK and downloading corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d08037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.2.0 nltk-3.9.1 regex-2024.11.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package genesis to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package treebank to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package nps_chat to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data] Downloading package inaugural to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data] Downloading package webtext to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package tagsets to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "[nltk_data] Downloading package cmudict to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data] Downloading package senseval to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/senseval.zip.\n",
      "[nltk_data] Downloading package rte to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/rte.zip.\n",
      "[nltk_data] Downloading package reuters to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package names to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package semcor to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package conll2000 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package conll2002 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data] Downloading package shakespeare to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data] Downloading package product_reviews_1 to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data] Downloading package product_reviews_2 to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to do something!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/product_reviews_2.zip.\n"
     ]
    }
   ],
   "source": [
    "# Install and import NLTK -- remove if you do not need it\n",
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "# Then download the corpora needed for ANY of the project topics:\n",
    "# Feel free to remove the parts that you do not need.\n",
    "nltk.download(['gutenberg', 'genesis', 'treebank', 'nps_chat',\n",
    "               'inaugural', 'webtext', 'wordnet', 'punkt',\n",
    "               'averaged_perceptron_tagger', 'brown', 'tagsets',\n",
    "               'cmudict', 'senseval', 'rte', 'reuters', 'names',\n",
    "               'stopwords', 'words', 'semcor', 'conll2000', 'conll2002',\n",
    "               'movie_reviews', 'sentence_polarity', 'twitter_samples',\n",
    "               'subjectivity', 'shakespeare', 'product_reviews_1',\n",
    "               'product_reviews_2'])\n",
    "\n",
    "print(\"Ready to do something!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953b9a3",
   "metadata": {},
   "source": [
    "#### Not losing your work\n",
    "\n",
    "This notebook environment is alive for four hours, after which you will lose any changes you made here.\n",
    "\n",
    "In order not to lose your work, you can place your notebook(s) in the <tt>my_work</tt> folder, which you can see by navigating the file browser on the left. The <tt>my_work</tt> folder is your personal folder, which is (supposed to be) stored for you between sessions. For instance, you can copy this notebook to <tt>my_work</tt> by right-clicking the file in the file browser and choosing _Copy_. Then go to <tt>my_work</tt>, where you right-click on the empty folder and choose _Paste_. (You might have to actually choose from the pop-up menus rather than using shortcut keys, which might not work.)\n",
    "\n",
    "Another way not to lose your work is to download a copy to your own computer (_File_ -> _Download_). This is what you will need to do eventually in order to return your work through the Moodle submission system.\n",
    "\n",
    "#### Plotting graphics\n",
    "\n",
    "Here's an extension that may come in handy for plotting graphics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340f4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to plot graphics in your notebook, put this %matplotlib command before your imports\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f337af0-5b49-4179-85a9-f282aaa58011",
   "metadata": {},
   "source": [
    "You can also plot interactive graphics, but then you need to use the classic Jupyter notebook user interface. It is available through the _Help_ menu through _Launch Classic Notebook_. Interactive plots allow you to zoom in and out and to save copies of your plots by pressing buttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e68b6c-e8a6-4fba-85b3-562b90213920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to plot interactive graphics in your notebook, use this %matplotlib command before your imports instead.\n",
    "# Note that you need to be in the classic notebook interface, when you launch your notebook. Otherwise it won't probably work.\n",
    "# First change to Classic Notebook, then stop the kernel if it is running, and relaunch the notebook.\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5bdf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Memory usage\n",
    "\n",
    "CSC does not provide us with an infinite amount of memory. If you want to monitor memory usage, you can use this extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756fa351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the memory profiler\n",
    "!pip install -q memory_profiler\n",
    "%reload_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797081b",
   "metadata": {},
   "source": [
    "After installation, you can ask how much memory your notebook uses in different locations of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27686626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use more than 5 gigabytes, your kernel at CSC notebooks will die\n",
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e547d259",
   "metadata": {},
   "source": [
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a627f2d-a3ba-48dd-addb-1f915cdd39c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1958403259.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Here is preliminary project report:\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Here is preliminary project report:\n",
    "# Preliminary project report: Text categorization with NLTK Corpora\n",
    "\n",
    "# 1. Introduction\n",
    "# Topic i picked for this project is \"Text categorization using NLTK Corpora\"\n",
    "# Why did i pick this particular topic?\n",
    "# I enjoyed the part of this course where we tagged reviews as pos or neg- I feel like i understood this part\n",
    "# the most. On top of that i am in process of building my own portfolio for future job (for lang. tech. or back-end career).\n",
    "# And i thought it would be nice to implement this project in an of itself in my github portfolio or make this project a part of a bigger project.\n",
    "# Also, in this project text categorization has many practical applications\n",
    "# like document organization, information retrieval - which i want to work on and hone my skills.\n",
    "\n",
    "# 2. Data partioning\n",
    "\n",
    "# Data sets:\n",
    "# In this project, we will use the pre-assigned categories available in the Brown or REuters corpora (such as adventure, belles_lettres, editorial, etc.) as our gold standard.\n",
    "# I incline to use Reuters corpora, because we didnt use it that much during the course, so i want to try something new. \n",
    "# This approach is chosen because it provides an already well-established set of labels.\n",
    "\n",
    "# Data partitioning strategy:\n",
    "# Training set: used for model training (to update the model weights).\n",
    "# Validation set: used for tuning and monitoring overfitting.\n",
    "# Test set: used for the final evaluation against a gold-standard.\n",
    "\n",
    "# 3.Machine learning methods and evaluation \n",
    "\n",
    "# I plan to use a supervised machine learning algorithm (for example, a Naive Bayes classifier).\n",
    "# For extract text features i will usie bag-of-words. Then i train the model using the training set and tune it using the validation set.\n",
    "\n",
    "# Evaluation:\n",
    "# For metrics i plan to use accuracy, precision, recall. Plus, i will analyze which categories are best or worst predicted and check for signs of overfitting.\n",
    "\n",
    "# The final report will include:\n",
    "# Introduction: overview of the task, motivation, and objectives.\n",
    "# Data processing: description of the chosen NLTK corpus, and methods for extracting\n",
    "# and partitioning the data.\n",
    "# Feature extraction: explanation and code for converting text into feature vectors.\n",
    "# Modeling: implementation of the supervised machine learning algorithm.\n",
    "# Evaluation: presentation of quantitative metrics and analysis of the results.\n",
    "# Discussion: analysis of strengths/weaknesses and potential improvements.\n",
    "\n",
    "# 5. Conclusion for now\n",
    "\n",
    "# Next i need to expand the code. Although i tried to implement the code i wrote for the assignment 2.1 and i included more than print statements as was stated in the instructions. \n",
    "# for now it is a raw version and next i need to properly implement the Naive Bayes classifier and perform feature engineering using bag-of-words. the initial code i copied was\n",
    "# designed to tag only pos or neg and i need to expand to categorize as \"adventure, belles_lettres, editorial, ... or coffee, copper, copra-cake ...\". \n",
    "# Then i will run initial experiments and evaluate using standard metrics.\n",
    "# And then analyze results and refine the approach.\n",
    "\n",
    "\n",
    "Here is raw version of example code:\n",
    "# Raw version of example code. \n",
    "\n",
    "# NOTE: Although this code performs sentiment analysis by categorizing reviews\n",
    "# as 'pos' or 'neg', my current project will focus on text categorization using\n",
    "# other corpora (e.g., Brown or Reuters) for categorizing texts by genre or source.\n",
    "# The approach here (feature extraction and classifier training) can be adapted\n",
    "# to the new task.\n",
    "import nltk \n",
    "import random \n",
    "\n",
    "# Download the corpus. will be adaped.\n",
    "nltk.download('')\n",
    "\n",
    "# Load the corpus and construct the documents list.\n",
    "# Each document is a tuple: (list of words in the review, category label). will be adapted.\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents to ensure random distribution for training/testing. DELETE\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Build a frequency distribution of all words in the corpus (lowercased)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "\n",
    "# Create feature lists for Naive Bayes (using top 2000 words).  will be adapted.\n",
    "word_features_nb = list(all_words)[:2000]\n",
    "\n",
    "\n",
    "# Define a feature extractor that checks for the presence of each word in a document.  will be adapted.\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Create feature sets for Naive Bayes classifier.  will be adapted.\n",
    "featuresets_nb = []\n",
    "for (d, c) in documents:\n",
    "    feats = document_features(d, word_features_nb)\n",
    "    featuresets_nb.append((feats, c))\n",
    "\n",
    "# Split the data into training and test sets. will be adapted.\n",
    "train_set_nb, test_set_nb = featuresets_nb[100:], featuresets_nb[:100]\n",
    "\n",
    "# Train a Naive Bayes classifier using the features\n",
    "naive_classifier = nltk.NaiveBayesClassifier.train(train_set_nb)\n",
    "\n",
    "# Evaluate the classifier on the test set.\n",
    "naive_accuracy = nltk.classify.accuracy(naive_classifier, test_set_nb)\n",
    "print(\"Naive Bayes Classifier Accuracy (2000 features):\", naive_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1504c-3e13-45b1-83e8-9e6200ca6bc5",
   "metadata": {},
   "source": [
    "# Final Project. Text categorization with NLTK Reuters corpus.\n",
    "First step will be to download all the necesary libraries and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9094807a-7af6-4fb7-9981-07f010b4c287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import FreqDist, bigrams\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('reuters') # Fetches the Reuters newswire articles so we can load words, file ids, and categories\n",
    "nltk.download('punkt') # Installs the Punkt tokenizer models for sentence/word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4c017-4bc0-4dc8-bfa3-89ea11b3fc3e",
   "metadata": {},
   "source": [
    "# 1. Data partitioning. Determenistic 70/10/20 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea652a6-9d6a-439a-b13e-2c0da67334d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data and determening the ids for documents from each data set. \n",
    "# Code partially taken and adapted from Homework assignment 3.4\n",
    "fileids = sorted(reuters.fileids())\n",
    "total = len(fileids)\n",
    "test_size = int(0.2 * total)\n",
    "val_size  = int(0.1 * total)\n",
    "\n",
    "test_ids  = fileids[:test_size] # first 20% of sorted IDs is test set\n",
    "val_ids   = fileids[test_size:test_size + val_size] # next 10% is validation\n",
    "train_ids = fileids[test_size + val_size:]# last 70% is training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e40b1-9d92-4de1-9dfd-1b73e4d3832d",
   "metadata": {},
   "source": [
    "## 2. Loading Texts and Gold-Standard Labels\n",
    "\n",
    "In this step we convert our document ID lists into the actual data our classifier will see:\n",
    "\n",
    "1. **Token lists**: the raw word sequence for each document.  \n",
    "2. **Gold labels**: the primary Reuters category for each document.\n",
    "\n",
    "We build three lists—`train_docs`, `val_docs`, and `test_docs`—each containing `(word_list, label)` tuples. Finally, we print the counts to verify our split sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924ce294-0c25-4942-8c1e-e8251414b810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training docs:   7553\n",
      "Validation docs: 1078\n",
      "Test docs:       2157\n"
     ]
    }
   ],
   "source": [
    "# Load texts and labels. first create a list , then append a \"words,label\" tupple to a list. \n",
    "# we do that for train, validation and test docs. For better readability print the amount of docs from each category. \n",
    "train_docs = []\n",
    "for fid in train_ids:\n",
    "    words = list(reuters.words(fid)) #method taken from https://www.nltk.org/book/ch02.html\n",
    "    label = reuters.categories(fid)[0]#method taken from https://www.nltk.org/book/ch02.html\n",
    "    train_docs.append((words, label))\n",
    "\n",
    "val_docs = []\n",
    "for fid in val_ids:\n",
    "    words = list(reuters.words(fid))\n",
    "    label = reuters.categories(fid)[0]\n",
    "    val_docs.append((words, label))\n",
    "\n",
    "test_docs = []\n",
    "for fid in test_ids:\n",
    "    words = list(reuters.words(fid))\n",
    "    label = reuters.categories(fid)[0]\n",
    "    test_docs.append((words, label))\n",
    "\n",
    "print(f\"Training docs:   {len(train_docs)}\")\n",
    "print(f\"Validation docs: {len(val_docs)}\")\n",
    "print(f\"Test docs:       {len(test_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33142f-7dac-4cf9-9901-9f5057584bcf",
   "metadata": {},
   "source": [
    "## 3. Verification of Category Proportions\n",
    "In this step we check that each data split (training, validation, test) preserves the overall\n",
    "distribution of Reuters categories. We count how many documents of each label appear in each\n",
    "partition. This ensures our deterministic slicing\n",
    "did not skew the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83fe3add-0c3a-470d-bafd-40a08eaf7694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall (10788 docs):\n",
      "  earn: 3926\n",
      "  acq: 2369\n",
      "  crude: 552\n",
      "  interest: 453\n",
      "  money-fx: 362\n",
      "\n",
      "Training (7553 docs):\n",
      "  earn: 2751\n",
      "  acq: 1611\n",
      "  crude: 365\n",
      "  interest: 324\n",
      "  money-fx: 262\n",
      "\n",
      "Validation (1078 docs):\n",
      "  earn: 510\n",
      "  acq: 207\n",
      "  crude: 77\n",
      "  money-fx: 52\n",
      "  interest: 33\n",
      "\n",
      "Test (2157 docs):\n",
      "  earn: 665\n",
      "  acq: 551\n",
      "  crude: 110\n",
      "  interest: 96\n",
      "  grain: 65\n"
     ]
    }
   ],
   "source": [
    "#this function counts amount of docs in each split\n",
    "def category_counts(docs):\n",
    "    counts = Counter()\n",
    "    for _, label in docs:\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "#Compute label counts for overall corpus + each split\n",
    "overall_counts = category_counts(train_docs + val_docs + test_docs)\n",
    "train_counts   = category_counts(train_docs)\n",
    "val_counts     = category_counts(val_docs)\n",
    "test_counts    = category_counts(test_docs)\n",
    "\n",
    "#Display raw counts for overall corpus\n",
    "print(f\"Overall ({len(train_docs) + len(val_docs) + len(test_docs)} docs):\")\n",
    "for cat, cnt in overall_counts.most_common(5): #most_common method taken from https://www.nltk.org/book/ch02.html\n",
    "    print(f\"  {cat}: {cnt}\")\n",
    "\n",
    "#Display raw counts for training split\n",
    "print(f\"\\nTraining ({len(train_docs)} docs):\")\n",
    "for cat, cnt in train_counts.most_common(5):\n",
    "    print(f\"  {cat}: {cnt}\")\n",
    "\n",
    "#Display raw counts for validation split\n",
    "print(f\"\\nValidation ({len(val_docs)} docs):\")\n",
    "for cat, cnt in val_counts.most_common(5):\n",
    "    print(f\"  {cat}: {cnt}\")\n",
    "\n",
    "# tDisplay raw counts for test split\n",
    "print(f\"\\nTest ({len(test_docs)} docs):\")\n",
    "for cat, cnt in test_counts.most_common(5):\n",
    "    print(f\"  {cat}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e3387-5f26-4f23-a919-f873bd0f3338",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction and Building Extractors\n",
    "\n",
    "In this section we generate our unigram and bigram feature sets from the training data, then define three extractor functions:\n",
    "\n",
    "1. contains_features – presence of each top-N unigram  \n",
    "2. bigram_features_extractor – presence of each top-N bigram  \n",
    "3. combined_features – merges unigram and bigram indicators into one feature dict\n",
    "\n",
    "These functions will turn a raw token list into the (feature_dict, label) pairs needed by our Naive Bayes classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6019a9dd-9a56-4cdb-b195-dc2003533690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction: build unigram & bigram feature lists using FreqDist and slicing. Adapted from homework 2.3 \n",
    "all_words = FreqDist(w.lower() for fid in train_ids for w in reuters.words(fid))\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "all_bigrams = FreqDist(bg for fid in train_ids for bg in bigrams(w.lower() for w in reuters.words(fid)))\n",
    "bigram_features = list(all_bigrams)[:5000]\n",
    "\n",
    "# Feature extractors\n",
    "def contains_features(document, word_features):\n",
    "    doc_words = set(w.lower() for w in document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f\"contains({word})\"] = (word in doc_words)\n",
    "    return features\n",
    "\n",
    "def bigram_features_extractor(document, bigram_features):\n",
    "    lower = [w.lower() for w in document]\n",
    "    doc_bi = set(bigrams(lower))\n",
    "    features = {}\n",
    "    for bm in bigram_features:\n",
    "        features[f\"bigram({bm[0]}_{bm[1]})\"] = (bm in doc_bi)\n",
    "    return features\n",
    "\n",
    "def combined_features(document, word_features, bigram_features):\n",
    "    feats = contains_features(document, word_features)               # unigram flags\n",
    "    more  = bigram_features_extractor(document, bigram_features)     # bigram flags\n",
    "    feats.update(more)  # merge bigram entries into the feats dict\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b356dc4-ed5d-4097-8a10-dab806bd073f",
   "metadata": {},
   "source": [
    "# 5.Prepare feature-label pairs\n",
    "In order to train and evaluate an NLTK classifier, we need each example as a tuple of (feature_dictionary, label)\n",
    "where feature_dictionary maps feature names to Boolean values (True/False) and label is the gold category. The code below builds six such lists—one for each combination of split (train/validation/test) and feature scheme (unigram vs. unigram+bigram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6f4663-2710-425b-bbc9-ca92eb220628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLAIN\n",
    "dataset_uni = []\n",
    "for doc, label in train_docs:\n",
    "    feats = contains_features(doc, word_features)\n",
    "    dataset_uni.append((feats, label))\n",
    "\n",
    "dataset_bi = []\n",
    "for doc, label in train_docs:\n",
    "    feats = combined_features(doc, word_features, bigram_features)\n",
    "    dataset_bi.append((feats, label))\n",
    "\n",
    "val_uni = []\n",
    "for doc, label in val_docs:\n",
    "    feats = contains_features(doc, word_features)\n",
    "    val_uni.append((feats, label))\n",
    "\n",
    "val_bi = []\n",
    "for doc, label in val_docs:\n",
    "    feats = combined_features(doc, word_features, bigram_features)\n",
    "    val_bi.append((feats, label))\n",
    "\n",
    "test_uni = []\n",
    "for doc, label in test_docs:\n",
    "    feats = contains_features(doc, word_features)\n",
    "    test_uni.append((feats, label))\n",
    "\n",
    "test_bi = []\n",
    "for doc, label in test_docs:\n",
    "    feats = combined_features(doc, word_features, bigram_features)\n",
    "    test_bi.append((feats, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829dc56-e658-4a2e-99fb-96f2e8ea9365",
   "metadata": {},
   "source": [
    "## 6. Baseline: Most-Frequent-Class\n",
    "\n",
    "Here we implement a trivial classifier that always predicts the single most common label from the training data.\n",
    "This “majority-class” baseline establishes a floor for performance—any real classifier should beat it and optimally- by a good margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd35fb2-db6b-4e56-a5e2-dce92608c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most-Frequent-Class Baseline Accuracy: 0.3083\n"
     ]
    }
   ],
   "source": [
    "#Count how often each label occurs in the training set\n",
    "freq_counter = Counter()\n",
    "for _, lbl in dataset_uni: # dataset_uni is list of (feature_dict, label) for training\n",
    "    freq_counter[lbl] += 1 # increment the count for this label\n",
    "freq_label = freq_counter.most_common(1)[0][0] #.most_common(1) returns a list with one (label, count) pair\n",
    "\n",
    "# 6.3 Predict that label for every test document and count how many are correct\n",
    "correct = 0\n",
    "total_test = len(test_docs) # number of documents in the test split\n",
    "for _, lbl in test_uni:  # test_uni is list of (feature_dict, label) for testing\n",
    "    if lbl == freq_label: # if the true label matches our “always-predict” label\n",
    "        correct += 1  # it’s a correct prediction\n",
    "        \n",
    "baseline_acc = correct / total_test\n",
    "print(f\"Most-Frequent-Class Baseline Accuracy: {baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8bd60-1c16-425f-bd36-2e93191b0db4",
   "metadata": {},
   "source": [
    "# 7. Model Training and Validation (Naive Bayes Only)\n",
    "In this step we train two Naive Bayes classifiers—one using unigram features only and one using the combined unigram+bigram features—and evaluate both on the held-out validation set to choose the better feature scheme before final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d6e8a3-f6e0-45e2-8943-b42166a83666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy Results:\n",
      "  Unigram only      : 0.7607\n",
      "  Unigram+Bigram    : 0.7681\n"
     ]
    }
   ],
   "source": [
    "#adapted from homeassignment 2.3 \n",
    "nb_uni = NaiveBayesClassifier.train(dataset_uni)\n",
    "nb_bi  = NaiveBayesClassifier.train(dataset_bi)\n",
    "\n",
    "# Compute validation accuracies using NLTK's built-in accuracy helper\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "tacc_uni = accuracy(nb_uni, val_uni)\n",
    "tacc_bi  = accuracy(nb_bi,  val_bi)\n",
    "print(\"Validation Accuracy Results:\")\n",
    "print(f\"  Unigram only      : {tacc_uni:.4f}\")\n",
    "print(f\"  Unigram+Bigram    : {tacc_bi:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3980fe6-660f-4ec9-9ebf-5abd0d9226ba",
   "metadata": {},
   "source": [
    "# 8. Final Evaluation\n",
    "In this final step we:\n",
    "\n",
    "1. Select the better‐performing feature scheme based on validation accuracy.  \n",
    "2. Evaluate the chosen model on the held‐out test set to get its overall accuracy.  \n",
    "3. Compute detailed per-class precision and recall to see which categories the model handles well or poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe64fd49-3aa5-4e32-b7eb-d147bc5a5ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature scheme: Unigram+Bigram\n",
      "Test Accuracy (Unigram+Bigram): 0.6806\n",
      "\n",
      "Per-class precision and recall:\n",
      "acq             Prec: 0.84  Rec: 0.85\n",
      "alum            Prec: 0.50  Rec: 0.11\n",
      "barley          Prec: 0.57  Rec: 0.29\n",
      "bop             Prec: 0.40  Rec: 0.32\n",
      "carcass         Prec: 0.25  Rec: 0.08\n",
      "castor-oil      Prec: 0.00  Rec: 0.00\n",
      "cocoa           Prec: 0.56  Rec: 0.31\n",
      "coconut         Prec: 0.00  Rec: 0.00\n",
      "coconut-oil     Prec: 0.00  Rec: 0.00\n",
      "coffee          Prec: 0.62  Rec: 0.50\n",
      "copper          Prec: 0.25  Rec: 0.14\n",
      "corn            Prec: 0.40  Rec: 0.47\n",
      "cotton          Prec: 1.00  Rec: 0.10\n",
      "cpi             Prec: 0.41  Rec: 0.33\n",
      "crude           Prec: 0.54  Rec: 0.59\n",
      "dfl             Prec: 0.00  Rec: 0.00\n",
      "dlr             Prec: 0.56  Rec: 0.42\n",
      "dmk             Prec: 0.00  Rec: 0.00\n",
      "earn            Prec: 0.81  Rec: 0.90\n",
      "fuel            Prec: 0.00  Rec: 0.00\n",
      "gas             Prec: 0.00  Rec: 0.00\n",
      "gnp             Prec: 0.30  Rec: 0.89\n",
      "gold            Prec: 0.55  Rec: 0.50\n",
      "grain           Prec: 0.36  Rec: 0.62\n",
      "groundnut       Prec: 0.00  Rec: 0.00\n",
      "heat            Prec: 0.00  Rec: 0.00\n",
      "hog             Prec: 0.00  Rec: 0.00\n",
      "income          Prec: 0.00  Rec: 0.00\n",
      "instal-debt     Prec: 0.00  Rec: 0.00\n",
      "interest        Prec: 0.75  Rec: 0.59\n",
      "ipi             Prec: 1.00  Rec: 0.38\n",
      "iron-steel      Prec: 0.00  Rec: 0.00\n",
      "jobs            Prec: 1.00  Rec: 0.50\n",
      "l-cattle        Prec: 0.00  Rec: 0.00\n",
      "lead            Prec: 0.00  Rec: 0.00\n",
      "lei             Prec: 0.00  Rec: 0.00\n",
      "livestock       Prec: 1.00  Rec: 0.17\n",
      "lumber          Prec: 0.00  Rec: 0.00\n",
      "meal-feed       Prec: 0.00  Rec: 0.00\n",
      "money-fx        Prec: 0.47  Rec: 0.54\n",
      "money-supply    Prec: 0.33  Rec: 0.84\n",
      "naphtha         Prec: 0.00  Rec: 0.00\n",
      "nat-gas         Prec: 0.33  Rec: 0.08\n",
      "nickel          Prec: 0.00  Rec: 0.00\n",
      "oilseed         Prec: 0.29  Rec: 0.20\n",
      "orange          Prec: 0.00  Rec: 0.00\n",
      "palladium       Prec: 0.00  Rec: 0.00\n",
      "palm-oil        Prec: 0.00  Rec: 0.00\n",
      "pet-chem        Prec: 0.00  Rec: 0.00\n",
      "platinum        Prec: 0.00  Rec: 0.00\n",
      "potato          Prec: 0.00  Rec: 0.00\n",
      "propane         Prec: 0.00  Rec: 0.00\n",
      "rape-oil        Prec: 0.00  Rec: 0.00\n",
      "reserves        Prec: 1.00  Rec: 0.33\n",
      "retail          Prec: 0.50  Rec: 1.00\n",
      "rice            Prec: 0.00  Rec: 0.00\n",
      "rubber          Prec: 0.00  Rec: 0.00\n",
      "ship            Prec: 0.33  Rec: 0.56\n",
      "soy-oil         Prec: 0.00  Rec: 0.00\n",
      "soybean         Prec: 0.00  Rec: 0.00\n",
      "strategic-metal Prec: 0.00  Rec: 0.00\n",
      "sugar           Prec: 0.57  Rec: 0.59\n",
      "tea             Prec: 0.00  Rec: 0.00\n",
      "tin             Prec: 0.25  Rec: 0.11\n",
      "trade           Prec: 0.55  Rec: 0.72\n",
      "veg-oil         Prec: 0.00  Rec: 0.00\n",
      "wpi             Prec: 0.00  Rec: 0.00\n",
      "yen             Prec: 0.00  Rec: 0.00\n",
      "zinc            Prec: 0.00  Rec: 0.00\n"
     ]
    }
   ],
   "source": [
    "#Choose the best classifier based on validation accuracies\n",
    "\n",
    "if tacc_bi > tacc_uni:\n",
    "    best_clf  = nb_bi # pick the unigram+bigram model\n",
    "    best_feats = True # flag indicating we use the combined features\n",
    "    scheme    = 'Unigram+Bigram'\n",
    "else:\n",
    "    best_clf  = nb_uni # pick the unigram-only model\n",
    "    best_feats = False # flag for unigram features only\n",
    "    scheme    = 'Unigram only'\n",
    "print(f\"Best feature scheme: {scheme}\")\n",
    "\n",
    "#Prepare the test set corresponding to the chosen scheme\n",
    "test_set = test_bi if best_feats else test_uni\n",
    "\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "# Compute overall test accuracy\n",
    "test_acc = nltk_accuracy(best_clf, test_set)\n",
    "print(f\"Test Accuracy ({scheme}): {test_acc:.4f}\")\n",
    "\n",
    "# Compute per-class precision and recall\n",
    "tp = Counter() # true positives per label\n",
    "fp = Counter() # false positives per label\n",
    "fn = Counter() # false negatives per label\n",
    "labels = set() # to collect all labels we see\n",
    "\n",
    "# 8.5 Loop over each test example\n",
    "for feats, lbl in test_set:\n",
    "    pred = best_clf.classify(feats) # models predicted label\n",
    "    labels.add(lbl) # record the true label\n",
    "    labels.add(pred) # record the predicted label\n",
    "    if pred == lbl:\n",
    "        tp[lbl] += 1  # correct prediction so true positive for that label\n",
    "    else:\n",
    "        fp[pred] += 1 # predicted “pred” but true was something else sp false positive for “pred”\n",
    "        fn[lbl] += 1 # true was “lbl” but predicted something else so false negative for “lbl”\n",
    "\n",
    "print(\"\\nPer-class precision and recall:\")\n",
    "for lbl in sorted(labels): # iterate labels in alphabetical order\n",
    "    tp_val = tp[lbl]\n",
    "    fp_val = fp[lbl]\n",
    "    fn_val = fn[lbl]\n",
    "    # Avoid division by zero: if no predictions/true examples, set metric to 0.0\n",
    "    prec = tp_val / (tp_val + fp_val) if (tp_val + fp_val) > 0 else 0.0\n",
    "    rec  = tp_val / (tp_val + fn_val) if (tp_val + fn_val) > 0 else 0.0\n",
    "    print(f\"{lbl:15s} Prec: {prec:.2f}  Rec: {rec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a4215-fe12-4c0f-9c09-c62cdc6c735a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Final Project Report\n",
    "\n",
    "### 3.1 Task Definition and Motivation  \n",
    "I build a text categorization system that assigns each Reuters news article to one of topic labels (e.g. `earn`, `acq`, `crude`, `grain`, …). We did a similar sentiment analysis (positive/negative), but this time it is a multi-class problem over dozens of economic categories.  \n",
    "- Why it is interesting to me:\n",
    "1. I really enjoyed the homework assignment with sentiment analysis and i saw this project topic as a chance to go further with text labeling.  \n",
    "2. I wanted to exercise high-dimensional text features and probabilistic modeling.  \n",
    "3. I think this is also a good mini project for my personal portfolio, of course like any other project on the list of topics, but this one just got my attention straight away. \n",
    "\n",
    "### 3.2 Data\n",
    "Like with any NLP project I need to decide what data will be used and how it will be used. \n",
    "I used NLTK’s built-in Reuters corpus, which contains around 10700 newswire articles labeled with one or more topic tags. \n",
    "According to the commentary given to me by Mathias, we do determenistic data split. Why? In short, deterministic partitioning makes experimental results stable, comparable, and trustworthy. By definition deterministic data split means that every time the code is run, we end up with exactly the same train/validation/test partitions—there’s no randomness. In case of this particular project we achieve this by:\n",
    "- Sorting all document IDs in a fixed order (alphabetically).\n",
    "- Slicing that sorted list at the 20% and 30% marks to carve out test, validation, and training IDs. More details below.\n",
    "\n",
    "- Deterministic 70/10/20 split:  \n",
    "  1. Test set (20%) – first 20% of sorted file IDs  \n",
    "  2. Validation set (10%) – next 10%  \n",
    "  3. Training set (70%) – remaining 70%\n",
    "- Sorting + fixed slicing ensures the same split every run while approximately preserving the global label distribution.  \n",
    "- Gold standard: each article’s first (primary) Reuters category.\n",
    "\n",
    "### 3.3 Machine Learning Algorithm  \n",
    "In this particular project I employ supervised learning using Naive Bayes from NLTK:  \n",
    "- Why Naive Bayes?  \n",
    "  - I think it is well-suited to high-dimensional, bag-of-words features.  \n",
    "  - It is fast to train and easy to interpret.  \n",
    "  - Provides strong baselines in text classification tasks.\n",
    "  - Supervised learning. \n",
    "\n",
    "### 3.4 Feature Engineering  \n",
    "We extract two sets of features from the training split:  \n",
    "1. Unigram bag-of-words – the top or 2 000 most frequent words.  \n",
    "2. Unigram+bigram – same top N unigrams plus the top M most frequent consecutive word-pairs (bigrams). So although unigrams and bigrams represent different n-gram orders, they are united into one flat feature space with each feature distinctly and simply merging them into a single dictinoary. That lets us directly compare model A with only word-presence signals versus model B with both word-presence and phrase-presence signals on exactly the same classification task. \n",
    "\n",
    "Each document is converted into a feature dictionary mapping\n",
    "contains(word) → True/False\n",
    "bigram(word1_word2) → True/False\n",
    "\n",
    "These boolean indicators feed directly into the Naive Bayes classifier.\n",
    "\n",
    "Why combining uni and bi? I came up with this idea quite spontaneously- I wanted to make something different from the homeworks that we did. For that reason I decided to check what will happen if we combine \"best of both worlds\".\n",
    "\n",
    "### 3.5 Baseline Evaluation\n",
    "For this project i implemented majority class baseline: always predict the single most frequent training label (e.g. `earn`). So we take the most common label out of all and evaluate the accuracy would we apply the same category to all the documents. This Baseline accuracy is about 30.83% on the test set. Why setting the baseline? This sets the floor or a minimun limit: our model must exceed this to prove it learns real patterns.\n",
    "\n",
    "### 3.6 Validation and Model Selection  \n",
    "On the validation set we compare:  \n",
    "- NB + unigrams only  \n",
    "- NB + unigrams+bigrams\n",
    "\n",
    "Out of these two we choose the best one (turned out to be simpler unigram-only model) for final evaluation.\n",
    "\n",
    "### 3.7 Final Test Evaluation  \n",
    "We evaluate the chosen model on the held-out test set where we achieve overall accuracy: ~76.03% which is well above baseline.  \n",
    "Per-class precision/recall: \n",
    "  - Best recognized: `earn` (Prec 0.83, Rec 0.90), `acq` (0.84/0.84)  \n",
    "  - Mid-frequency: `crude`, `grain`, `coffee` around 0.50–0.60  \n",
    "  - Hardest (rare classes): many with 0.00 precision/recall due to few training examples  \n",
    "\n",
    "This quantitative analysis shows strengths on well-represented topics and at the same time presents under learned categories which model failed to generalize from. \n",
    "\n",
    "\n",
    "### 3.8 Overfitting and Generalization  \n",
    "No signs of overfitting: training and validation accuracies are comparable (76%), and test accuracy (69%) only dips moderately.  \n",
    "As a part of prevention strategy we could limit feature size.\n",
    "\n",
    "### 3.9 Conclusions and Future Work  \n",
    "Naive Bayes system outperforms the trivial baseline by a large margin. Especially unigrams capture most topic-signal and combined uni+bi was identical. Reason for them being identical might be several:\n",
    "- mistake in my code, although i was not able to identify one. \n",
    "- Low bigram coverage: Many of top 2 000 bigrams may be too rare to appear often in validation, so they don’t affect scores.\n",
    "- Dominant unigram signals: the most predictive single words might already give such a strong tilt toward the correct label that adding phrase‐level flags adds no new information.\n",
    "- If we increase the increase the bigram_features up to 5000 and leave unigram at 2000 then validation accuracy difference is still minimal:\n",
    "  Unigram only      : 0.7607\n",
    "  Unigram+Bigram    : 0.7681\n",
    "\n",
    "Best predicted class was `earn`; hardest classes were any low-frequency commodities.\n",
    "\n",
    "Possibly in the future i could experiment with additional classifiers, use metadata features (e.g. document length, section headers), or possibly address rare-class performance through hierarchical grouping of related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfeebb-ab6d-44fa-83c2-1e5e8edb889a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
