{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c1504c-3e13-45b1-83e8-9e6200ca6bc5",
   "metadata": {},
   "source": [
    "# Final Project. Text categorization with NLTK Reuters corpus.\n",
    "First step will be to download all the necesary libraries and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094807a-7af6-4fb7-9981-07f010b4c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import FreqDist, bigrams\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('reuters') # Fetches the Reuters newswire articles so we can load words, file ids, and categories\n",
    "nltk.download('punkt') # Installs the Punkt tokenizer models for sentence/word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4c017-4bc0-4dc8-bfa3-89ea11b3fc3e",
   "metadata": {},
   "source": [
    "# 1. Data partitioning. Determenistic 70/10/20 sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea652a6-9d6a-439a-b13e-2c0da67334d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data and determening the ids for documents from each data set. \n",
    "# Code partially taken and adapted from Homework assignment 3.4\n",
    "fileids = sorted(reuters.fileids())\n",
    "total = len(fileids)\n",
    "test_size = int(0.2 * total)\n",
    "val_size  = int(0.1 * total)\n",
    "\n",
    "test_ids  = fileids[:test_size] # first 20% of sorted IDs is test set\n",
    "val_ids   = fileids[test_size:test_size + val_size] # next 10% is validation\n",
    "train_ids = fileids[test_size + val_size:]# last 70% is training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e40b1-9d92-4de1-9dfd-1b73e4d3832d",
   "metadata": {},
   "source": [
    "## 2. Loading Texts and Gold-Standard Labels\n",
    "\n",
    "In this step we convert our document ID lists into the actual data our classifier will see:\n",
    "\n",
    "1. **Token lists**: the raw word sequence for each document.  \n",
    "2. **Gold labels**: the primary Reuters category for each document.\n",
    "\n",
    "We build three lists—`train_docs`, `val_docs`, and `test_docs`—each containing `(word_list, label)` tuples. Finally, we print the counts to verify our split sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ce294-0c25-4942-8c1e-e8251414b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load texts and labels. first create a list , then append a \"words,label\" tupple to a list. \n",
    "# we do that for train, validation and test docs. For better readability print the amount of docs from each category. \n",
    "train_docs = []\n",
    "for fid in train_ids:\n",
    "    words = list(reuters.words(fid)) #method taken from https://www.nltk.org/book/ch02.html\n",
    "    label = reuters.categories(fid)[0]#method taken from https://www.nltk.org/book/ch02.html\n",
    "    train_docs.append((words, label))\n",
    "\n",
    "val_docs = []\n",
    "for fid in val_ids:\n",
    "    words = list(reuters.words(fid))\n",
    "    label = reuters.categories(fid)[0]\n",
    "    val_docs.append((words, label))\n",
    "\n",
    "test_docs = []\n",
    "for fid in test_ids:\n",
    "    words = list(reuters.words(fid))\n",
    "    label = reuters.categories(fid)[0]\n",
    "    test_docs.append((words, label))\n",
    "\n",
    "print(f\"Training docs:   {len(train_docs)}\")\n",
    "print(f\"Validation docs: {len(val_docs)}\")\n",
    "print(f\"Test docs:       {len(test_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33142f-7dac-4cf9-9901-9f5057584bcf",
   "metadata": {},
   "source": [
    "## 3. Verification of Category Proportions\n",
    "In this step we check that each data split (training, validation, test) preserves the overall\n",
    "distribution of Reuters categories. We count how many documents of each label appear in each\n",
    "partition. This ensures our deterministic slicing\n",
    "did not skew the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe3add-0c3a-470d-bafd-40a08eaf7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function counts amount of docs in each split\n",
    "def category_counts(docs):\n",
    "    counts = Counter()\n",
    "    for _, label in docs:\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "#Compute label counts for overall corpus + each split\n",
    "overall_counts = category_counts(train_docs + val_docs + test_docs)\n",
    "train_counts   = category_counts(train_docs)\n",
    "val_counts     = category_counts(val_docs)\n",
    "test_counts    = category_counts(test_docs)\n",
    "\n",
    "#Display raw counts for overall corpus\n",
    "print(f\"Overall ({len(train_docs) + len(val_docs) + len(test_docs)} docs):\")\n",
    "for cat, cnt in overall_counts.most_common(5): #most_common method taken from https://www.nltk.org/book/ch02.html\n",
    "    print(f\"  {cat}: {cnt}\")\n",
    "\n",
    "#Display raw counts for training split\n",
    "print(f\"\\nTraining ({len(train_docs)} docs):\")\n",
    "for cat, cnt in train_counts.most_common(5):\n",
    "    print(f\"  {cat}: {cnt}\")\n",
    "\n",
    "#Display raw counts for validation split\n",
    "print(f\"\\nValidation ({len(val_docs)} docs):\")\n",
    "for cat, cnt in val_counts.most_common(5):\n",
    "    print(f\"  {cat}: {cnt}\")\n",
    "\n",
    "# tDisplay raw counts for test split\n",
    "print(f\"\\nTest ({len(test_docs)} docs):\")\n",
    "for cat, cnt in test_counts.most_common(5):\n",
    "    print(f\"  {cat}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e3387-5f26-4f23-a919-f873bd0f3338",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction and Building Extractors\n",
    "\n",
    "In this section we generate our unigram and bigram feature sets from the training data, then define three extractor functions:\n",
    "\n",
    "1. contains_features – presence of each top-N unigram  \n",
    "2. bigram_features_extractor – presence of each top-N bigram  \n",
    "3. combined_features – merges unigram and bigram indicators into one feature dict\n",
    "\n",
    "These functions will turn a raw token list into the (feature_dict, label) pairs needed by our Naive Bayes classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6019a9dd-9a56-4cdb-b195-dc2003533690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction: build unigram & bigram feature lists using FreqDist and slicing. Adapted from homework 2.3 \n",
    "all_words = FreqDist(w.lower() for fid in train_ids for w in reuters.words(fid))\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "all_bigrams = FreqDist(bg for fid in train_ids for bg in bigrams(w.lower() for w in reuters.words(fid)))\n",
    "bigram_features = list(all_bigrams)[:5000]\n",
    "\n",
    "# Feature extractors\n",
    "def contains_features(document, word_features):\n",
    "    doc_words = set(w.lower() for w in document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f\"contains({word})\"] = (word in doc_words)\n",
    "    return features\n",
    "\n",
    "def bigram_features_extractor(document, bigram_features):\n",
    "    lower = [w.lower() for w in document]\n",
    "    doc_bi = set(bigrams(lower))\n",
    "    features = {}\n",
    "    for bm in bigram_features:\n",
    "        features[f\"bigram({bm[0]}_{bm[1]})\"] = (bm in doc_bi)\n",
    "    return features\n",
    "\n",
    "def combined_features(document, word_features, bigram_features):\n",
    "    feats = contains_features(document, word_features)               # unigram flags\n",
    "    more  = bigram_features_extractor(document, bigram_features)     # bigram flags\n",
    "    feats.update(more)  # merge bigram entries into the feats dict\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b356dc4-ed5d-4097-8a10-dab806bd073f",
   "metadata": {},
   "source": [
    "# 5.Prepare feature-label pairs\n",
    "In order to train and evaluate an NLTK classifier, we need each example as a tuple of (feature_dictionary, label)\n",
    "where feature_dictionary maps feature names to Boolean values (True/False) and label is the gold category. The code below builds six such lists—one for each combination of split (train/validation/test) and feature scheme (unigram vs. unigram+bigram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6f4663-2710-425b-bbc9-ca92eb220628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLAIN\n",
    "dataset_uni = []\n",
    "for doc, label in train_docs:\n",
    "    feats = contains_features(doc, word_features)\n",
    "    dataset_uni.append((feats, label))\n",
    "\n",
    "dataset_bi = []\n",
    "for doc, label in train_docs:\n",
    "    feats = combined_features(doc, word_features, bigram_features)\n",
    "    dataset_bi.append((feats, label))\n",
    "\n",
    "val_uni = []\n",
    "for doc, label in val_docs:\n",
    "    feats = contains_features(doc, word_features)\n",
    "    val_uni.append((feats, label))\n",
    "\n",
    "val_bi = []\n",
    "for doc, label in val_docs:\n",
    "    feats = combined_features(doc, word_features, bigram_features)\n",
    "    val_bi.append((feats, label))\n",
    "\n",
    "test_uni = []\n",
    "for doc, label in test_docs:\n",
    "    feats = contains_features(doc, word_features)\n",
    "    test_uni.append((feats, label))\n",
    "\n",
    "test_bi = []\n",
    "for doc, label in test_docs:\n",
    "    feats = combined_features(doc, word_features, bigram_features)\n",
    "    test_bi.append((feats, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829dc56-e658-4a2e-99fb-96f2e8ea9365",
   "metadata": {},
   "source": [
    "## 6. Baseline: Most-Frequent-Class\n",
    "\n",
    "Here we implement a trivial classifier that always predicts the single most common label from the training data.\n",
    "This “majority-class” baseline establishes a floor for performance—any real classifier should beat it and optimally- by a good margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd35fb2-db6b-4e56-a5e2-dce92608c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count how often each label occurs in the training set\n",
    "freq_counter = Counter()\n",
    "for _, lbl in dataset_uni: # dataset_uni is list of (feature_dict, label) for training\n",
    "    freq_counter[lbl] += 1 # increment the count for this label\n",
    "freq_label = freq_counter.most_common(1)[0][0] #.most_common(1) returns a list with one (label, count) pair\n",
    "\n",
    "# 6.3 Predict that label for every test document and count how many are correct\n",
    "correct = 0\n",
    "total_test = len(test_docs) # number of documents in the test split\n",
    "for _, lbl in test_uni:  # test_uni is list of (feature_dict, label) for testing\n",
    "    if lbl == freq_label: # if the true label matches our “always-predict” label\n",
    "        correct += 1  # it’s a correct prediction\n",
    "        \n",
    "baseline_acc = correct / total_test\n",
    "print(f\"Most-Frequent-Class Baseline Accuracy: {baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8bd60-1c16-425f-bd36-2e93191b0db4",
   "metadata": {},
   "source": [
    "# 7. Model Training and Validation (Naive Bayes Only)\n",
    "In this step we train two Naive Bayes classifiers—one using unigram features only and one using the combined unigram+bigram features—and evaluate both on the held-out validation set to choose the better feature scheme before final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6e8a3-f6e0-45e2-8943-b42166a83666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from homeassignment 2.3 \n",
    "nb_uni = NaiveBayesClassifier.train(dataset_uni)\n",
    "nb_bi  = NaiveBayesClassifier.train(dataset_bi)\n",
    "\n",
    "# Compute validation accuracies using NLTK's built-in accuracy helper\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "tacc_uni = accuracy(nb_uni, val_uni)\n",
    "tacc_bi  = accuracy(nb_bi,  val_bi)\n",
    "print(\"Validation Accuracy Results:\")\n",
    "print(f\"  Unigram only      : {tacc_uni:.4f}\")\n",
    "print(f\"  Unigram+Bigram    : {tacc_bi:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3980fe6-660f-4ec9-9ebf-5abd0d9226ba",
   "metadata": {},
   "source": [
    "# 8. Final Evaluation\n",
    "In this final step we:\n",
    "\n",
    "1. Select the better‐performing feature scheme based on validation accuracy.  \n",
    "2. Evaluate the chosen model on the held‐out test set to get its overall accuracy.  \n",
    "3. Compute detailed per-class precision and recall to see which categories the model handles well or poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64fd49-3aa5-4e32-b7eb-d147bc5a5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the best classifier based on validation accuracies\n",
    "\n",
    "if tacc_bi > tacc_uni:\n",
    "    best_clf  = nb_bi # pick the unigram+bigram model\n",
    "    best_feats = True # flag indicating we use the combined features\n",
    "    scheme    = 'Unigram+Bigram'\n",
    "else:\n",
    "    best_clf  = nb_uni # pick the unigram-only model\n",
    "    best_feats = False # flag for unigram features only\n",
    "    scheme    = 'Unigram only'\n",
    "print(f\"Best feature scheme: {scheme}\")\n",
    "\n",
    "#Prepare the test set corresponding to the chosen scheme\n",
    "test_set = test_bi if best_feats else test_uni\n",
    "\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "# Compute overall test accuracy\n",
    "test_acc = nltk_accuracy(best_clf, test_set)\n",
    "print(f\"Test Accuracy ({scheme}): {test_acc:.4f}\")\n",
    "\n",
    "# Compute per-class precision and recall\n",
    "tp = Counter() # true positives per label\n",
    "fp = Counter() # false positives per label\n",
    "fn = Counter() # false negatives per label\n",
    "labels = set() # to collect all labels we see\n",
    "\n",
    "# 8.5 Loop over each test example\n",
    "for feats, lbl in test_set:\n",
    "    pred = best_clf.classify(feats) # models predicted label\n",
    "    labels.add(lbl) # record the true label\n",
    "    labels.add(pred) # record the predicted label\n",
    "    if pred == lbl:\n",
    "        tp[lbl] += 1  # correct prediction so true positive for that label\n",
    "    else:\n",
    "        fp[pred] += 1 # predicted “pred” but true was something else sp false positive for “pred”\n",
    "        fn[lbl] += 1 # true was “lbl” but predicted something else so false negative for “lbl”\n",
    "\n",
    "print(\"\\nPer-class precision and recall:\")\n",
    "for lbl in sorted(labels): # iterate labels in alphabetical order\n",
    "    tp_val = tp[lbl]\n",
    "    fp_val = fp[lbl]\n",
    "    fn_val = fn[lbl]\n",
    "    # Avoid division by zero: if no predictions/true examples, set metric to 0.0\n",
    "    prec = tp_val / (tp_val + fp_val) if (tp_val + fp_val) > 0 else 0.0\n",
    "    rec  = tp_val / (tp_val + fn_val) if (tp_val + fn_val) > 0 else 0.0\n",
    "    print(f\"{lbl:15s} Prec: {prec:.2f}  Rec: {rec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a4215-fe12-4c0f-9c09-c62cdc6c735a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Final Project Report\n",
    "\n",
    "### 3.1 Task Definition and Motivation  \n",
    "I build a text categorization system that assigns each Reuters news article to one of topic labels (e.g. `earn`, `acq`, `crude`, `grain`, …). We did a similar sentiment analysis (positive/negative), but this time it is a multi-class problem over dozens of economic categories.  \n",
    "- Why it is interesting to me:\n",
    "1. I really enjoyed the homework assignment with sentiment analysis and i saw this project topic as a chance to go further with text labeling.  \n",
    "2. I wanted to exercise high-dimensional text features and probabilistic modeling.  \n",
    "3. I think this is also a good mini project for my personal portfolio, of course like any other project on the list of topics, but this one just got my attention straight away. \n",
    "\n",
    "### 3.2 Data\n",
    "Like with any NLP project I need to decide what data will be used and how it will be used. \n",
    "I used NLTK’s built-in Reuters corpus, which contains around 10700 newswire articles labeled with one or more topic tags. \n",
    "According to the commentary given to me by Mathias, we do determenistic data split. Why? In short, deterministic partitioning makes experimental results stable, comparable, and trustworthy. By definition deterministic data split means that every time the code is run, we end up with exactly the same train/validation/test partitions—there’s no randomness. In case of this particular project we achieve this by:\n",
    "- Sorting all document IDs in a fixed order (alphabetically).\n",
    "- Slicing that sorted list at the 20% and 30% marks to carve out test, validation, and training IDs. More details below.\n",
    "\n",
    "- Deterministic 70/10/20 split:  \n",
    "  1. Test set (20%) – first 20% of sorted file IDs  \n",
    "  2. Validation set (10%) – next 10%  \n",
    "  3. Training set (70%) – remaining 70%\n",
    "- Sorting + fixed slicing ensures the same split every run while approximately preserving the global label distribution.  \n",
    "- Gold standard: each article’s first (primary) Reuters category.\n",
    "\n",
    "### 3.3 Machine Learning Algorithm  \n",
    "In this particular project I employ supervised learning using Naive Bayes from NLTK:  \n",
    "- Why Naive Bayes?  \n",
    "  - I think it is well-suited to high-dimensional, bag-of-words features.  \n",
    "  - It is fast to train and easy to interpret.  \n",
    "  - Provides strong baselines in text classification tasks.\n",
    "  - Supervised learning. \n",
    "\n",
    "### 3.4 Feature Engineering  \n",
    "We extract two sets of features from the training split:  \n",
    "1. Unigram bag-of-words – the top or 2 000 most frequent words.  \n",
    "2. Unigram+bigram – same top N unigrams plus the top M most frequent consecutive word-pairs (bigrams). So although unigrams and bigrams represent different n-gram orders, they are united into one flat feature space with each feature distinctly and simply merging them into a single dictinoary. That lets us directly compare model A with only word-presence signals versus model B with both word-presence and phrase-presence signals on exactly the same classification task. \n",
    "\n",
    "Each document is converted into a feature dictionary mapping\n",
    "contains(word) → True/False\n",
    "bigram(word1_word2) → True/False\n",
    "\n",
    "These boolean indicators feed directly into the Naive Bayes classifier.\n",
    "\n",
    "Why combining uni and bi? I came up with this idea quite spontaneously- I wanted to make something different from the homeworks that we did. For that reason I decided to check what will happen if we combine \"best of both worlds\".\n",
    "\n",
    "### 3.5 Baseline Evaluation\n",
    "For this project i implemented majority class baseline: always predict the single most frequent training label (e.g. `earn`). So we take the most common label out of all and evaluate the accuracy would we apply the same category to all the documents. This Baseline accuracy is about 30.83% on the test set. Why setting the baseline? This sets the floor or a minimun limit: our model must exceed this to prove it learns real patterns.\n",
    "\n",
    "### 3.6 Validation and Model Selection  \n",
    "On the validation set we compare:  \n",
    "- NB + unigrams only  \n",
    "- NB + unigrams+bigrams\n",
    "\n",
    "Out of these two we choose the best one (turned out to be simpler unigram-only model) for final evaluation.\n",
    "\n",
    "### 3.7 Final Test Evaluation  \n",
    "We evaluate the chosen model on the held-out test set where we achieve overall accuracy: ~76.03% which is well above baseline.  \n",
    "Per-class precision/recall: \n",
    "  - Best recognized: `earn` (Prec 0.83, Rec 0.90), `acq` (0.84/0.84)  \n",
    "  - Mid-frequency: `crude`, `grain`, `coffee` around 0.50–0.60  \n",
    "  - Hardest (rare classes): many with 0.00 precision/recall due to few training examples  \n",
    "\n",
    "This quantitative analysis shows strengths on well-represented topics and at the same time presents under learned categories which model failed to generalize from. \n",
    "\n",
    "\n",
    "### 3.8 Overfitting and Generalization  \n",
    "No signs of overfitting: training and validation accuracies are comparable (76%), and test accuracy (69%) only dips moderately.  \n",
    "As a part of prevention strategy we could limit feature size.\n",
    "\n",
    "### 3.9 Conclusions and Future Work  \n",
    "Naive Bayes system outperforms the trivial baseline by a large margin. Especially unigrams capture most topic-signal and combined uni+bi was identical. Reason for them being identical might be several:\n",
    "- mistake in my code, although i was not able to identify one. \n",
    "- Low bigram coverage: Many of top 2 000 bigrams may be too rare to appear often in validation, so they don’t affect scores.\n",
    "- Dominant unigram signals: the most predictive single words might already give such a strong tilt toward the correct label that adding phrase‐level flags adds no new information.\n",
    "- If we increase the increase the bigram_features up to 5000 and leave unigram at 2000 then validation accuracy difference is still minimal:\n",
    "  Unigram only      : 0.7607\n",
    "  Unigram+Bigram    : 0.7681\n",
    "\n",
    "Best predicted class was `earn`; hardest classes were any low-frequency commodities.\n",
    "\n",
    "Possibly in the future i could experiment with additional classifiers, use metadata features (e.g. document length, section headers), or possibly address rare-class performance through hierarchical grouping of related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfeebb-ab6d-44fa-83c2-1e5e8edb889a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
